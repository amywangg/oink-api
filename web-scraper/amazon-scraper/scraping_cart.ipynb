{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "def scrape_cart():\n",
    "    try:\n",
    "        # from here on we are going to use our own html code because we can't seem to open and add something to shopping cart through our script\n",
    "        urllib.request.urlretrieve(\"https://www.amazon.ca/gp/cart/view.html?ref_=nav_cart\", \"123123.txt\")\n",
    "        file = \"123123.txt\"\n",
    "        our_page = codecs.open(file, 'r') \n",
    "        soup = BeautifulSoup(our_page, 'html.parser')\n",
    "\n",
    "\n",
    "        # all products in cart are within this div tag\n",
    "        products = soup.findAll(\"div\",{\"class\":\"sc-list-item-content\"})\n",
    "\n",
    "        # initialize dictionary for all product info\n",
    "        appendix={}\n",
    "\n",
    "        # loop through all products in the cart and get their link, item code, name, price\n",
    "        for each in products:\n",
    "            link = \"https://www.amazon.ca\" + each.div.find(\"a\",{\"class\":\"a-link-normal sc-product-link\"})['href']\n",
    "\n",
    "            sub_code= re.sub(r'^.*?product/', '', link)\n",
    "            code = sub_code[:sub_code.find(\"/ref\")]\n",
    "\n",
    "            # categories will be added in next sprint when we deal with Amazon API\n",
    "            # item_link_response = requests.get('https://api.rainforestapi.com/request?api_key=B4057000DCF0420D8944FE2D7CFCF6B9&type=product&amazon_domain=amazon.ca&asin=' + code)\n",
    "            # category = []\n",
    "            # for each in item_link_response.json()['product']['categories']:\n",
    "                # category.append(each['name'])\n",
    "            \n",
    "            name = re.sub('\\s+',' ',each.div.find(\"span\",{\"class\":\"a-size-medium sc-product-title a-text-bold\"}).text)\n",
    "            price = re.sub('\\s+',' ',each.div.find(\"span\",{\"class\":\"a-size-medium a-color-base sc-price sc-white-space-nowrap sc-product-price a-text-bold\"}).text)\n",
    "\n",
    "\n",
    "            appendix.update({\n",
    "                code:{\n",
    "                    \"link\" : link,\n",
    "                    \"name\" : name,\n",
    "                    \"price\" : price,\n",
    "                    # \"category\" : category\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        print(appendix)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_cart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "https://www.youtube.com/watch?v=mKxFfjNyj3c&ab_channel=edureka%21\n",
    "https://www.dataquest.io/blog/web-scraping-beautifulsoup/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
